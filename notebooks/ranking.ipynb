{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_similarity(title, image, description):\n",
    "    URL_FORMAT = f\"https://www.google.com/search?&q={title}&tbm=isch\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# URL of the web page containing the image\n",
    "url = f\"https://search.brave.com/images?q=amul%20bread\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "print(url)\n",
    "# Find all image tags\n",
    "print(soup.prettify())\n",
    "# main_div = soup.find(\"img\", _class=\"svelte-hfp50 loaded\")\n",
    "# print(main_div)\n",
    "# image_tags = soup.find(\"img\", _class=\"svelte-hfp50 loaded\")\n",
    "# print(image_tags)\n",
    "# Directory to save the images\n",
    "save_dir = \"images\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Download and save each image\n",
    "for img in image_tags:\n",
    "    img_url = img[\"src\"]\n",
    "    # Download the image\n",
    "    img_response = requests.get(img_url)\n",
    "    # Extract the image filename from the URL\n",
    "    img_filename = os.path.join(save_dir, os.path.basename(img_url))\n",
    "    # Save the image\n",
    "    with open(img_filename, \"wb\") as f:\n",
    "        f.write(img_response.content)\n",
    "        print(f\"Image '{img_filename}' saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "from PIL import Image\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "def get_images_from_google(url):\n",
    "    wd = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    wd.get(url)\n",
    "    # image_url = None\n",
    "\n",
    "    thumbnail = wd.find_element(By.CSS_SELECTOR, \"Q4LuWd\")\n",
    "    # thumbnail.click()\n",
    "    # image_url = wd.find_element(By.CLASS_NAME, \"iPVvYb\")\n",
    "    # image_url = wd.find_element(By.CLASS_NAME, \"sFlh5c pT0Scc iPVvYb\")\n",
    "\n",
    "    print(thumbnail)\n",
    "    time.sleep(1)\n",
    "    wd.quit()\n",
    "\n",
    "    # return imgResults\n",
    "\n",
    "\n",
    "def download_image(download_path, url, file_name):\n",
    "    image_content = requests.get(url).content\n",
    "    image_file = io.BytesIO(image_content)\n",
    "    image = Image.open(image_file)\n",
    "    file_path = download_path + file_name\n",
    "\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        image.save(f, \"jpeg\")\n",
    "\n",
    "    print(\"done\")\n",
    "\n",
    "\n",
    "print(get_images_from_google(f\"https://search.brave.com/images?q=amul%20bread\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "download_image(\"./images/\", \"https://m.media-amazon.com/images/I/71+TmE0ZekL.jpg\", \"test.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 11s 11s/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "Similarity score: 0.17009449\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    return img_array\n",
    "\n",
    "\n",
    "def extract_features(img_path, model):\n",
    "    img = preprocess_image(img_path)\n",
    "    features = model.predict(img)\n",
    "    return features.flatten()\n",
    "\n",
    "\n",
    "def calculate_similarity(features1, features2):\n",
    "    similarity_score = cosine_similarity([features1], [features2])\n",
    "    return similarity_score[0][0]\n",
    "\n",
    "\n",
    "# Load pre-trained VGG16 model\n",
    "model = VGG16(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# Path to the images\n",
    "image1_path = r\"src\\test_amul_bread.jpeg\"\n",
    "# image2_path = r\"src\\brit_bread.jpeg\"\n",
    "# image1_path = r\"src\\brit_bread2.jpeg\"\n",
    "image2_path = r\"src\\butter_bread_sandwich_1.jpeg\"\n",
    "\n",
    "# Extract features\n",
    "features1 = extract_features(image1_path, model)\n",
    "features2 = extract_features(image2_path, model)\n",
    "\n",
    "# Calculate similarity score\n",
    "similarity_score = calculate_similarity(features1, features2)\n",
    "print(\"Similarity score:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "def check_img_similarity(query, product_image):\n",
    "    def get_ref_img_url(query):\n",
    "    # Prepare Google search URL\n",
    "        query = \"+\".join(query.split())\n",
    "        url = f\"https://www.google.com/search?q={query}&tbm=isch\"\n",
    "\n",
    "        # Send HTTP request and get response\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract the URL of the second image\n",
    "        img_tags = soup.find_all(\"img\")\n",
    "        img_url = img_tags[1][\"src\"]\n",
    "        # if len(img_tags) >= 2:\n",
    "            # return img_url\n",
    "\n",
    "        def download_image(download_path, url, file_name):\n",
    "            image_content = requests.get(url).content\n",
    "            image_file = io.BytesIO(image_content)\n",
    "            image = Image.open(image_file)\n",
    "            file_path = download_path + file_name\n",
    "\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                image.save(f, \"jpeg\")\n",
    "\n",
    "            print(\"done\")\n",
    "            \n",
    "        download_image(\"./images/\", img_url, \"test1.jpg\")\n",
    "\n",
    "\n",
    "    get_ref_img_url(query)\n",
    "    path1 = product_image\n",
    "    path2 = \"./images/test1.jpg\"\n",
    "\n",
    "    def preprocess_image(img_path):\n",
    "        img = image.load_img(img_path, target_size=(224, 224))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        return img_array\n",
    "\n",
    "    def extract_features(img_path, model):\n",
    "        img = preprocess_image(img_path)\n",
    "        features = model.predict(img)\n",
    "        return features.flatten()\n",
    "\n",
    "    def calculate_similarity(features1, features2):\n",
    "        similarity_score = cosine_similarity([features1], [features2])\n",
    "        return similarity_score[0][0]\n",
    "\n",
    "    # Load pre-trained VGG16 model\n",
    "    model = VGG16(weights=\"imagenet\", include_top=False)\n",
    "    features1 = extract_features(path1, model)\n",
    "    features2 = extract_features(path2, model)\n",
    "\n",
    "    # Calculate similarity score\n",
    "    similarity_score = calculate_similarity(features1, features2)\n",
    "    print(\"Similarity score:\", similarity_score)\n",
    "    \n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "1/1 [==============================] - 0s 264ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Similarity score: 0.6413834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6413834"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_image = r\"src\\brit_bread2.jpeg\" #image of britannia bread with brittania reference\n",
    "check_img_similarity(\"brittania brown bed\", product_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Similarity score: 0.20311785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20311785"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_image = r\"src\\brit_bread2.jpeg\" #image of britannia bread with amul bread reference\n",
    "check_img_similarity(\"Amul brown bread\", product_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
